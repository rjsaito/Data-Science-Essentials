{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjsaito/Data-Science-Essentials/blob/master/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5pM_oluMCl",
        "colab_type": "text"
      },
      "source": [
        "# Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRuPkCOflVGb",
        "colab_type": "text"
      },
      "source": [
        "## Eigenvalues and Eigenvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VSwqhaOq-8T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHrWaPTylXC3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "$$\n",
        "A v = \\lambda v\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di4LY-Otq_1y",
        "colab_type": "text"
      },
      "source": [
        "- eigen value \n",
        "- eigen vector\n",
        "- invertibe matrix\n",
        "- singularity\n",
        "- singular values (square root of eigen values)\n",
        "- rank\n",
        "- linear independence\n",
        "- singular value decomposition\n",
        "- eigenvalue decomposition (matrix represented in eigenvalues and eigenvectors)\n",
        "- singular value decomposition (complexity of m^2 n + n^3, or O(mn^2))\n",
        "- matrix factorization (nm)^(O(2^r r^2)) time exact solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Bh1vWPOIbe",
        "colab_type": "text"
      },
      "source": [
        "## Principal Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0YP9mnJOLRp",
        "colab_type": "text"
      },
      "source": [
        "## Factor Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv2Hjn6-lQaC",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gf9JZgJrKU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEYHDJCDnmJS",
        "colab_type": "text"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpQO5LZKnp1n",
        "colab_type": "text"
      },
      "source": [
        "### Cross Entropy (Log Loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4NJUETXnr4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Full function: -(y*log(p) + (1-y)*log(1-p))\n",
        "def CrossEntropy(p, y):\n",
        "  if y == 1:\n",
        "    return -log(p)\n",
        "  else:\n",
        "    return -log(1 - p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZFhvxXNoof-",
        "colab_type": "text"
      },
      "source": [
        "### Hinge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPMQYkyion9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Hinge(p, y):\n",
        "  return np.max(0, 1 - yHat * y)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l9vMafio7AP",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error (L1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D4sf1b6o8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MAE(yH, y):\n",
        "  return np.sum(np.absolute(yH - y)) / y.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZszRc3qpHsQ",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error (L2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy2GgxJkpKON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MSE(yH, y):\n",
        "  return np.sum((yH - y)**2) / y.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqmB8fWuOYn",
        "colab_type": "text"
      },
      "source": [
        "## scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2jHO6lOSxji",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG88zDi9ReK1",
        "colab_type": "text"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMR3ksRRgUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d31ad3a0-1519-4e78-8d80-6d29746752f6"
      },
      "source": [
        "# load libraries\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# load data\n",
        "boston = load_boston()\n",
        "features = boston.data\n",
        "target = boston.target\n",
        "\n",
        "# standardize\n",
        "scalar = StandardScaler()\n",
        "features_standardized = scalar.fit_transform(features)\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G-Pvxf5SZjr",
        "colab_type": "text"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yakAQlX1SdVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# polynomial terms\n",
        "polynomial = PolynomialFeatures(degree = 3, include_bias = False)\n",
        "features_polynomial = polynomial.fit_transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdr2tSMvSs3D",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OhZH46SSsJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create regression\n",
        "regression = LinearRegression()\n",
        "\n",
        "# fit model\n",
        "model = regression.fit(features_polynomial, target)\n",
        "\n",
        "# get model output\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKzR_EVoTT53",
        "colab_type": "text"
      },
      "source": [
        "### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZUeihkpTVVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a15c4562-04cf-4345-851e-9448b4b219eb"
      },
      "source": [
        "# create ridge regression, with cross validation\n",
        "ridge = RidgeCV(cv = 5)\n",
        "\n",
        "# fit model\n",
        "ridgemodel = ridge.fit(features, target)\n",
        "\n",
        "# get model output\n",
        "print(ridgemodel.intercept_)\n",
        "print(ridgemodel.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27.467884964141177\n",
            "[-0.10143535  0.0495791  -0.0429624   1.95202082 -2.37161896  3.70227207\n",
            " -0.01070735 -1.24880821  0.2795956  -0.01399313 -0.79794498  0.01003684\n",
            " -0.55936642]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWCxwVi8Va7Y",
        "colab_type": "text"
      },
      "source": [
        "### Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-tEy6hdVede",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0530986b-cdfc-4b2e-b6e0-3fcfdace126f"
      },
      "source": [
        "# create lasso regression, with cross validation\n",
        "lasso = LassoCV(cv = 5)\n",
        "\n",
        "# fit model\n",
        "lassomodel = lasso.fit(features, target)\n",
        "\n",
        "# get model output\n",
        "print(lassomodel.intercept_)\n",
        "print(lassomodel.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36.33499969015174\n",
            "[-0.07426626  0.04945448 -0.          0.         -0.          1.804385\n",
            "  0.01133345 -0.81324404  0.27228399 -0.01542465 -0.74287183  0.00892587\n",
            " -0.70365352]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hpjbd9YS6Mc",
        "colab_type": "text"
      },
      "source": [
        "## Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKexvCyS9vv",
        "colab_type": "text"
      },
      "source": [
        "### LibMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw0w4VbnS_lB",
        "colab_type": "text"
      },
      "source": [
        "https://www.csie.ntu.edu.tw/~cjlin/papers/libmf/libmf_open_source.pdf\n",
        "\n",
        "Non-convex optimization problem:\n",
        "\n",
        "$$ \\underset{P,Q}{min} \\underset{(u,v)\\epsilon R}{\\Sigma} [f(p_u, q_v; r_{u,v}) + \\mu_p || p_u||_1 + \\mu_q ||q_v||_1 +\n",
        "\\frac{λ_p}{2} || p_u ||^2_2 + \\frac{λ_q}{2} ||q_v||^2_2 ]  \\ \\ \\ (1)\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$f(p_u, q_v; r_{u,v})$ is the loss function, $p_u, q_v$ are latent factors, $r_{u,v}$ is the interaction, and $\\mu_p, \\mu_q, λ_p, λ_q$ are regularization parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz6KLDTlXBd5",
        "colab_type": "text"
      },
      "source": [
        "### Training with Stochastic Gradient Descent\n",
        "\n",
        "The algorithm for the Fast Parallelized Stochastic Gradient Descent is:\n",
        "\n",
        "1. randomly shuffle R\n",
        "2. grid R into a set B with at least (s + 1) × (s + 1) blocks\n",
        "3. sort each block by user (or item) identities\n",
        "4. construct a scheduler\n",
        "5. launch s working threads\n",
        "6. wait until the total number of updates reaches a user-defined value\n",
        "\n",
        "\n",
        "The basic idea of SG is that, instead of expensively calculating the gradient of (1), it randomly selects a $(u,v)$ entry from the summation and calculates the corresponding gradient [Robbins and Monro 1951; Kiefer and Wolfowitz 1952]. Once $r_{u,v}$ is chosen, the objective function in (1), is:\n",
        "\n",
        "$$ f(p_u, q_v; r_{u,v}) + \\mu_p p_u + \\mu_q q_v +\n",
        "\\frac{λ_p}{2} p_u^T p_u + \\frac{λ_q}{2} q_v^T q_v \\ \\ \\ (2)\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "We calculate the sub-gradient over $p_u$ and $q_v$. Variables are updated by the following rules:\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$\n",
        "p_u ← p_u + γ (\\frac{d}{dp_u}(2)),  \\\\\n",
        "q_v ← q_v + γ (\\frac{d}{dq_v}(2))\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkFNJJX1ZuZ2",
        "colab_type": "text"
      },
      "source": [
        "### Loss Functions\n",
        "\n",
        " $f(·)$ is a non-convex loss function of $p_u$ and $q_v$, and $\\mu_p$, $\\mu_q$, $\\lambda_p$, and $\\lambda_q$ are regularization coefficients. For Real Valued MF, the loss function can be a squared loss, an absolute loss, or generalized KL-divergence. If R is a binary matrix, users may select among logistic loss, hinge loss, and squared hinge loss to perform BMF. Note that, the non-negative constraints,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-avBcjXobjcU",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w6Zb_FgcdbA",
        "colab_type": "text"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivkBvXscfkZ",
        "colab_type": "text"
      },
      "source": [
        "### Exploding Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3a-iNQnrFpT",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdgOGhkirHRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []                              # to keep track of the cost\n",
        "    m = X.shape[1]                           # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "    \n",
        "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        A1, cache1 = linear_activation_forward(X, W1, b1, activation = \"relu\")\n",
        "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = \"sigmoid\")\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        cost = compute_cost(A2, Y)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "        \n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n",
        "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (approx. 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "        \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "       \n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4iX2v_FWELb",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyYLn_gfaJlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia5j8_JaXgnK",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCEC_wFz4bw3",
        "colab_type": "text"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydPWDMJf4hPH",
        "colab_type": "text"
      },
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdhB21Wn2zov",
        "colab_type": "text"
      },
      "source": [
        "RNN for Short-Term Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtD9RRf0Xj-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "print(\"Tensorflow version: \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHuOUZub27ny",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Display utilities [RUN ME]\n",
        "\n",
        "from enum import IntEnum\n",
        "import numpy as np\n",
        "\n",
        "class Waveforms(IntEnum):\n",
        "    SINE1 = 0\n",
        "    SINE2 = 1\n",
        "    SINE3 = 2\n",
        "    SINE4 = 3\n",
        "\n",
        "def create_time_series(waveform, datalen):\n",
        "    # Generates a sequence of length datalen\n",
        "    # There are three available waveforms in the Waveforms enum\n",
        "    # good waveforms\n",
        "    frequencies = [(0.2, 0.15), (0.35, 0.3), (0.6, 0.55), (0.4, 0.25)]\n",
        "    freq1, freq2 = frequencies[waveform]\n",
        "    noise = [np.random.random()*0.2 for i in range(datalen)]\n",
        "    x1 = np.sin(np.arange(0,datalen) * freq1)  + noise\n",
        "    x2 = np.sin(np.arange(0,datalen) * freq2)  + noise\n",
        "    x = x1 + x2\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "from matplotlib import transforms as plttrans\n",
        "\n",
        "plt.rcParams['figure.figsize']=(16.8,6.0)\n",
        "plt.rcParams['axes.grid']=True\n",
        "plt.rcParams['axes.linewidth']=0\n",
        "plt.rcParams['grid.color']='#DDDDDD'\n",
        "plt.rcParams['axes.facecolor']='white'\n",
        "plt.rcParams['xtick.major.size']=0\n",
        "plt.rcParams['ytick.major.size']=0\n",
        "\n",
        "def picture_this_1(data, datalen):\n",
        "    plt.subplot(211)\n",
        "    plt.plot(data[datalen-512:datalen+512])\n",
        "    plt.axvspan(0, 512, color='black', alpha=0.06)\n",
        "    plt.axvspan(512, 1024, color='grey', alpha=0.04)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(data[3*datalen-512:3*datalen+512])\n",
        "    plt.axvspan(0, 512, color='grey', alpha=0.04)\n",
        "    plt.axvspan(512, 1024, color='black', alpha=0.06)\n",
        "    plt.show()\n",
        "    \n",
        "def picture_this_2(data, batchsize, seqlen):\n",
        "    samples = np.reshape(data, [-1, batchsize, seqlen])\n",
        "    rndsample = samples[np.random.choice(samples.shape[0], 8, replace=False)]\n",
        "    print(\"Tensor shape of a batch of training sequences: \" + str(rndsample[0].shape))\n",
        "    print(\"Random excerpt:\")\n",
        "    subplot = 241\n",
        "    for i in range(8):\n",
        "        plt.subplot(subplot)\n",
        "        plt.plot(rndsample[i, 0]) # first sequence in random batch\n",
        "        subplot += 1\n",
        "    plt.show()\n",
        "    \n",
        "def picture_this_3(predictions, evaldata, evallabels, seqlen):\n",
        "    subplot = 241\n",
        "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "    for i in range(8):\n",
        "        plt.subplot(subplot)\n",
        "        #k = int(np.random.rand() * evaldata.shape[0])\n",
        "        l0, = plt.plot(evaldata[i, 1:], label=\"data\")\n",
        "        plt.plot([seqlen-2, seqlen-1], evallabels[i, -2:])\n",
        "        l1, = plt.plot([seqlen-1], [predictions[i]], \"o\", color=\"red\", label='Predicted')\n",
        "        l2, = plt.plot([seqlen-1], [evallabels[i][-1]], \"o\", color=colors[1], label='Ground Truth')\n",
        "        if i==0:\n",
        "            plt.legend(handles=[l0, l1, l2])\n",
        "        subplot += 1\n",
        "    plt.show()\n",
        "    \n",
        "def picture_this_hist(rmse1, rmse2, rmse3, rmse):\n",
        "  colors = ['#4285f4', '#34a853', '#fbbc05', '#ea4334']\n",
        "  plt.figure(figsize=(5,4))\n",
        "  plt.xticks(rotation='40')\n",
        "  plt.title('RMSE: your model vs. simplistic approaches')\n",
        "  plt.bar(['RND', 'LAST', 'LAST2', 'Yours'], [rmse1, rmse2, rmse3, rmse], color=colors)\n",
        "  plt.show()\n",
        "\n",
        "def picture_this_hist_all(rmse1, rmse2, rmse3, rmse4, rmse5, rmse6, rmse7, rmse8):\n",
        "  colors = ['#4285f4', '#34a853', '#fbbc05', '#ea4334', '#4285f4', '#34a853', '#fbbc05', '#ea4334']\n",
        "  plt.figure(figsize=(7,4))\n",
        "  plt.xticks(rotation='40')\n",
        "  plt.ylim(0, 0.35)\n",
        "  plt.title('RMSE: all models')\n",
        "  plt.bar(['RND', 'LAST', 'LAST2', 'LINEAR', 'DNN', 'CNN', 'RNN', 'RNN_N'],\n",
        "          [rmse1, rmse2, rmse3, rmse4, rmse5, rmse6, rmse7, rmse8], color=colors)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCr691hT3Bnx",
        "colab_type": "text"
      },
      "source": [
        "Generate Fake Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DIlFYT83A3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_SEQ_LEN = 1024*128\n",
        "data = np.concatenate([create_time_series(waveform, DATA_SEQ_LEN) for waveform in Waveforms]) # 4 different wave forms\n",
        "picture_this_1(data, DATA_SEQ_LEN)\n",
        "DATA_LEN = DATA_SEQ_LEN * 4 # since we concatenated 4 sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZ1kGsi3FEx",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBh1cUHS3GGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNN_CELLSIZE = 32   # size of the RNN cells\n",
        "SEQLEN = 32         # unrolled sequence length\n",
        "BATCHSIZE = 32      # mini-batch size\n",
        "LAST_N = SEQLEN//2  # loss computed on last N element of sequence in advanced RNN model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkWC9wPP3JBA",
        "colab_type": "text"
      },
      "source": [
        "Visualize Training Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN5aFmYH3KFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "picture_this_2(data, BATCHSIZE, SEQLEN) # execute multiple times to see different sample sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl4HORqh3OJ1",
        "colab_type": "text"
      },
      "source": [
        "Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CobkBsfj3P08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is how to create a Keras model from neural network layers\n",
        "def compile_keras_sequential_model(list_of_layers, msg):\n",
        "  \n",
        "    # a tf.keras.Sequential model is a sequence of layers\n",
        "    model = tf.keras.Sequential(list_of_layers)\n",
        "    \n",
        "    # keras does not have a pre-defined metric for Root Mean Square Error. Let's define one.\n",
        "    def rmse(y_true, y_pred): # Root Mean Squared Error\n",
        "      return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "    \n",
        "    print('\\nModel ', msg)\n",
        "    \n",
        "    # to finalize the model, specify the loss, the optimizer and metrics\n",
        "    model.compile(\n",
        "       loss = 'mean_squared_error',\n",
        "       optimizer = 'rmsprop',\n",
        "       metrics = [rmse])\n",
        "    \n",
        "    # this prints a description of the model\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "  \n",
        "#\n",
        "# three very simplistic \"models\" that require no training. Can you beat them ?\n",
        "#\n",
        "\n",
        "# SIMPLISTIC BENCHMARK MODEL 1\n",
        "predict_same_as_last_value = lambda x: x[:,-1] # shape of x is [BATCHSIZE,SEQLEN]\n",
        "# SIMPLISTIC BENCHMARK MODEL 2\n",
        "predict_trend_from_last_two_values = lambda x: x[:,-1] + (x[:,-1] - x[:,-2])\n",
        "# SIMPLISTIC BENCHMARK MODEL 3\n",
        "predict_random_value = lambda x: tf.random.uniform(tf.shape(x)[0:1], -2.0, 2.0)\n",
        "\n",
        "def model_layers_from_lambda(lambda_fn, input_shape, output_shape):\n",
        "  return [tf.keras.layers.Lambda(lambda_fn, input_shape=input_shape),\n",
        "          tf.keras.layers.Reshape(output_shape)]\n",
        "\n",
        "model_layers_RAND  = model_layers_from_lambda(predict_random_value,               input_shape=[SEQLEN,], output_shape=[1,])\n",
        "model_layers_LAST  = model_layers_from_lambda(predict_same_as_last_value,         input_shape=[SEQLEN,], output_shape=[1,])\n",
        "model_layers_LAST2 = model_layers_from_lambda(predict_trend_from_last_two_values, input_shape=[SEQLEN,], output_shape=[1,])\n",
        "\n",
        "# three neural network models for comparison, in increasing order of complexity\n",
        "\n",
        "l = tf.keras.layers  # syntax shortcut\n",
        "\n",
        "# BENCHMARK MODEL 4: linear model (RMSE: 0.215 after 10 epochs)\n",
        "model_layers_LINEAR = [l.Dense(1, input_shape=[SEQLEN,])] # output shape [BATCHSIZE, 1]\n",
        "\n",
        "# BENCHMARK MODEL 5: 2-layer dense model (RMSE: 0.197 after 10 epochs)\n",
        "model_layers_DNN = [l.Dense(SEQLEN//2, activation='relu', input_shape=[SEQLEN,]), # input  shape [BATCHSIZE, SEQLEN]\n",
        "                    l.Dense(1)] # output shape [BATCHSIZE, 1]\n",
        "\n",
        "# BENCHMARK MODEL 6: convolutional (RMSE: 0.186 after 10 epochs)\n",
        "model_layers_CNN = [\n",
        "    l.Reshape([SEQLEN, 1], input_shape=[SEQLEN,]), # [BATCHSIZE, SEQLEN, 1] is necessary for conv model\n",
        "    l.Conv1D(filters=8, kernel_size=4, activation='relu', padding=\"same\"), # [BATCHSIZE, SEQLEN, 8]\n",
        "    l.Conv1D(filters=16, kernel_size=3, activation='relu', padding=\"same\"), # [BATCHSIZE, SEQLEN, 8]\n",
        "    l.Conv1D(filters=8, kernel_size=1, activation='relu', padding=\"same\"), # [BATCHSIZE, SEQLEN, 8]\n",
        "    l.MaxPooling1D(pool_size=2, strides=2),  # [BATCHSIZE, SEQLEN//2, 8]\n",
        "    l.Conv1D(filters=8, kernel_size=3, activation='relu', padding=\"same\"),  # [BATCHSIZE, SEQLEN//2, 8]\n",
        "    l.MaxPooling1D(pool_size=2, strides=2),  # [BATCHSIZE, SEQLEN//4, 8]\n",
        "    # mis-using a conv layer as linear regression :-)\n",
        "    l.Conv1D(filters=1, kernel_size=SEQLEN//4, activation=None, padding=\"valid\"), # output shape [BATCHSIZE, 1, 1]\n",
        "    l.Reshape([1,]) ] # output shape [BATCHSIZE, 1]\n",
        "\n",
        "# RNN\n",
        "model_layers_RNN = [\n",
        "    # input shape needed on first layer only\n",
        "    l.Reshape([SEQLEN, 1], input_shape=[SEQLEN,]),\n",
        "    l.GRU(RNN_CELLSIZE), # shape [BATCHSIZE, RNN_CELLSIZE]\n",
        "    l.Dense(1, ) # shape [BATCHSIZE, 1]\n",
        "    \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLdkfDMu4nDI",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw0rFkuUDGNT",
        "colab_type": "text"
      },
      "source": [
        "#### **Types of CV Problems**\n",
        "\n",
        "- Image Classifcation\n",
        "- Object Detection\n",
        "- Neural Style Transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ineuf_hDdOV",
        "colab_type": "text"
      },
      "source": [
        "#### **Edge Detection**\n",
        "\n",
        "Edges are detected using Filters (or Kernels)\n",
        "- In Tensorflow: tf.nn.conv2d\n",
        "- In Keras: Cond2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAISj84XEMF0",
        "colab_type": "text"
      },
      "source": [
        "Horizontal Edge Detection (3x3 filter)\n",
        "\n",
        "|  1,  1,  1 | <br/>\n",
        "|  0,  0,  0 | <br/>\n",
        "| -1, -1, -1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsYm49XEOLG",
        "colab_type": "text"
      },
      "source": [
        "Vertical Edge Detection (3x3 filter)\n",
        "\n",
        "| 1, 0, -1 | <br/>\n",
        "| 1, 0, -1 | <br/>\n",
        "| 1, 0, -1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-WVHzxZE2Qi",
        "colab_type": "text"
      },
      "source": [
        "Sobol Filter (3x3)\n",
        "- More robust (vertical) detector with higher weight in the center\n",
        "\n",
        "| 1, 0, -1 | <br/>\n",
        "| 2, 0, -2 | <br/>\n",
        "| 1, 0, -1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxokzfBsFSn4",
        "colab_type": "text"
      },
      "source": [
        "#### **Padding**\n",
        "\n",
        "Padding is used to ensure the input array size matches the output (so as not to lose the edge pixels)\n",
        "\n",
        "Two Types of Convolution:\n",
        "- Valid: no filter (nxn * fxf -> [n - f + 1] x [n - f + 1])\n",
        "- Same: pad so output size is the same as input (padding p = (f - 1) / 2\n",
        "\n",
        "*Note: f is usually odd (1x1, 3x3, 5x5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgFfaRCGUUJ",
        "colab_type": "text"
      },
      "source": [
        "#### **Strided Convolution**\n",
        "\n",
        "Stride is the # of steps (pixels) to move over at each filter iteration (vertically and horizontally)\n",
        "\n",
        " (nxn * fxf, stride s = 2  -> [(n + 2p - f)/s + 1] x [(n + 2p - f)/s + 1] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20pJrd_hHw9R",
        "colab_type": "text"
      },
      "source": [
        "#### **Convolution on RGB images**\n",
        "\n",
        "Your input may look like 6 x 6 x 3 (height x width x color channels)\n",
        "\n",
        "Your convolution (filter) may then be a 3 x 3 x 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJHGuTNnJufn",
        "colab_type": "text"
      },
      "source": [
        "#### **Pooling Layers**\n",
        "\n",
        "Pooling will abstract or summarise your input (bring down to lower resolution)\n",
        "\n",
        "Types of Pooling:\n",
        "- Max Pooling\n",
        "- Average Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZz4d9tR4cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPuaWnOeK6HB",
        "colab_type": "text"
      },
      "source": [
        "#### **Why use Convolution?**\n",
        "\n",
        "- **Parameter sharing**: A feature detector (such as vertical edge detector) that's useful in one part of the iomage is probably useful in another part of the image\n",
        "- **Sparsity of connections**: In each layer, each output value depends only on a small number of inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg_tdgwaRPwP",
        "colab_type": "text"
      },
      "source": [
        "#### **\\# of Parameters and Shape**\n",
        "\n",
        "- Conv Layer:\n",
        "  - \\# Parameters: ([ width m ] x [ height n ] x [prev layer's filters d] + bias 1) * [k filters in current layer)\n",
        "  - Shape: [(n + 2 \\* padding - filters)/stride + 1] x [(n + 2 \\* padding - filters)/stride + 1] x ? )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjR00-KVDhL",
        "colab_type": "text"
      },
      "source": [
        "| Type | Dimensions |\n",
        "|--|--|\n",
        "| Input | nh[L-1] x nw[L-1] x nc[L-1]|\n",
        "| Activation a[l] | nh[L] x nw[L] x nc[L] |\n",
        "| Weights | f[L] x f[L] x nc[L-1] x nc[L]|\n",
        "| Bias | nc[L]|\n",
        "\n",
        "Where\n",
        "- f[L] = filter size\n",
        "- p[L] = padding\n",
        "- s]L] = stride\n",
        "- nc[L] = number of filters\n",
        "- nc[L-1] = color channels\n",
        "\n",
        "https://engmrk.com/convolutional-neural-network-3/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNbYk6r4oyT",
        "colab_type": "text"
      },
      "source": [
        "### **Classic Architectures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhgSXoJoOB3",
        "colab_type": "text"
      },
      "source": [
        "**LeNet-5 (1998)**\n",
        "\n",
        "1. Convolution 6 [5x5] (s = 1)\n",
        "2. Avg Pooling (f = 2, s = 2)\n",
        "3. Convolution 16 [5x5] (s = 1)\n",
        "4. Avg Pooling (f = 2, s = 2)\n",
        "5. Dense Layer (120 nodes)\n",
        "6. Dense Layer (84 nodes)\n",
        "7. Softmax (10-way)\n",
        "\n",
        "~60K parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chRLnXbao8Up",
        "colab_type": "text"
      },
      "source": [
        "**AlexNet (2012)**\n",
        "\n",
        "0. Start: 227x227x3\n",
        "1. Convolution 96 [11x11] (s = 4)\n",
        "2. Max Pooling (f = 3, s = 2)\n",
        "3. Convolution 256 [5x5] (same)\n",
        "3. Max Pooling (f = 3, s = 2)\n",
        "5. Convolution 384 [3x3] (same)\n",
        "6. Convolution 384 [3x3] (same)\n",
        "7. Convolution 256 [3x3] (same)\n",
        "8. Max Pooling (f = 3, s = 2)\n",
        "9. Dense (4096)\n",
        "10. Dense (4096)\n",
        "11. Softmax (1000)\n",
        "\n",
        "~60M parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNANLopZccs-",
        "colab_type": "text"
      },
      "source": [
        "**VGG-16 (2015)**\n",
        "\n",
        "Key Features:\n",
        "- Conv = [3x3] (s = 1, same padding)\n",
        "- All Max Pooling [2x2] w/ stride = 2\n",
        "\n",
        "0. Start: 224x224x3\n",
        "1. Convolution 64x2 (2 times) [2x2] (same)\n",
        "2. Max Pooling (f = 2, s = 2)\n",
        "3. Convolution 128x2 (s = 1, same)\n",
        "4. Max Pooling (f = 2, s = 2)\n",
        "5. Convolution 256x3 (s = 1, same)\n",
        "6. Max Pooling (f = 2, s = 2)\n",
        "7. Convolution 512x3 (s = 1, same)\n",
        "8. Max Pooling (f = 2, s = 2)\n",
        "9. Convolution 512x3 (s = 1, same)\n",
        "10. Max Pooling (f - 2, s = 2)\n",
        "11. Dense (4096)\n",
        "12. Dense (4096)\n",
        "13. Softmax (1000)\n",
        "\n",
        "~138M parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhIaIgz_eKG_",
        "colab_type": "text"
      },
      "source": [
        "**ResNet (2015)**\n",
        "\n",
        "Key Features:\n",
        "- Utilizes Skip Networks \n",
        "- Utilizes Residuial block\n",
        "- BY training w/ Residual Block, training error does NOT increase with more layers\n",
        "\n",
        "Residual Block\n",
        "- Start w/ block A\n",
        "- Linear Operator (z = WA + b)\n",
        "- ReLu (A[L+1] = g(z))\n",
        "- Linear Operator (z2 = WA[L+1] + b)\n",
        "- ReLu w shortcut A (A[L+2] = g(z2 + A))"
      ]
    }
  ]
}